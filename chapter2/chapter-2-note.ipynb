{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90de011d",
   "metadata": {},
   "source": [
    "# What is Byte-Pair Encoding (BPE)?\n",
    "\n",
    "BPE is a tokenization algorithm widely used in modern NLP, particularly in large language models (LLMs), to break text into manageable units (tokens). Unlike traditional tokenization, which treats words or characters as tokens, BPE learns a vocabulary of tokens from a training corpus. These tokens can be words, subwords (like morphemes or smaller units), or even individual characters. The key advantage of BPE is its ability to handle unknown words by representing them as combinations of known subword units.\n",
    "\n",
    "## Why is BPE Useful?\n",
    "\n",
    "### Unknown Words\n",
    "In NLP, models trained on one corpus (training data) may encounter unseen words in a test corpus. For example, if the training data has low, new, and newer but not lower, BPE ensures lower can be tokenized as low + er, allowing the model to process it.\n",
    "\n",
    "### Flexibility\n",
    "BPE creates a vocabulary that balances whole words and subword units, making it robust for various languages and text types.\n",
    "\n",
    "### Efficiency\n",
    "It reduces vocabulary size compared to character-level tokenization while maintaining the ability to represent any word.\n",
    "\n",
    "# How Does BPE Work?\n",
    "\n",
    "BPE consists of two main components:\n",
    "\n",
    "1. **Token Learner**: Builds a vocabulary by iteratively merging the most frequent pairs of adjacent symbols in a training corpus.\n",
    "2. **Token Segmenter**: Uses the learned vocabulary to tokenize new (test) text.\n",
    "\n",
    "Here's a step-by-step explanation of the process, using the example from the book:\n",
    "\n",
    "## 1. Token Learner\n",
    "\n",
    "The token learner starts with a corpus and builds a vocabulary through iterative merging.\n",
    "\n",
    "### Input Corpus (example from the book):\n",
    "\n",
    "```text\n",
    "5 low\n",
    "2 lowest\n",
    "6 newer\n",
    "3 wider\n",
    "2 new\n",
    "```\n",
    "\n",
    "Each word is split into individual characters, with a special end-of-word symbol (`</w>` in some implementations, but here represented as `_` for simplicity). The initial vocabulary is the set of all unique characters:\n",
    "\n",
    "```text\n",
    "Vocabulary: [, d, e, i, l, n, o, r, s, t, w]\n",
    "```\n",
    "\n",
    "### Steps:\n",
    "\n",
    "- **Count Pairs**: Identify the most frequent pair of adjacent symbols in the corpus. For example, `e r` appears 9 times (6 in newer, 3 in wider).\n",
    "- **Merge**: Combine the most frequent pair into a single token (e.g., `er`). Update the corpus by replacing all instances of `e r` with `er`.\n",
    "- **Update Vocabulary**: Add the new token (`er`) to the vocabulary.\n",
    "- **Repeat**: Continue merging the most frequent pair for a specified number of iterations (k), where k is a parameter.\n",
    "\n",
    "### Example Merges (from the book):\n",
    "\n",
    "1. Merge `e r` → `er` (9 occurrences). Vocabulary becomes: `[, d, e, i, l, n, o, r, s, t, w, er]`.\n",
    "2. Merge `er *` → `er*` (word-final er). Vocabulary: `[, d, e, i, l, n, o, r, s, t, w, er, er_]`.\n",
    "3. Merge `n e` → `ne`. Vocabulary: `[, d, e, i, l, n, o, r, s, t, w, er, er_, ne]`.\n",
    "4. Continue merging (e.g., `ne w` → `new`, `l o` → `lo`, etc.) until k merges are complete.\n",
    "\n",
    "### Result\n",
    "The final vocabulary contains:\n",
    "- Original characters.\n",
    "- New tokens (subwords or full words) created by merging, like `er`, `ne`, `new`, `low`, `newer`.\n",
    "\n",
    "## 2. Token Segmenter\n",
    "\n",
    "The segmenter applies the learned vocabulary to tokenize new text (test data):\n",
    "\n",
    "- Split the test text into individual characters.\n",
    "- Apply the merges learned during training in the same order (greedy application).\n",
    "- For each merge, replace the corresponding pair of tokens with the merged token if it exists in the vocabulary.\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Test word: newer**\n",
    "- Initial: `n e w e r`\n",
    "- Apply merge 1: `n e w er` (since `e r` → `er`).\n",
    "- Apply merge 2: `n e w er_` (since `er *` → `er*`).\n",
    "- Apply merge 3: `ne w er_` (since `n e` → `ne`).\n",
    "- Apply later merges: Eventually, `newer` becomes a single token `newer`.\n",
    "\n",
    "**Test word: lower (unknown word)**\n",
    "- Initial: `l o w e r`\n",
    "- Apply merges: Results in `low er` (since `low` and `er` are in the vocabulary).\n",
    "\n",
    "The segmenter ensures that even unknown words are tokenized into known subword units, solving the unknown word problem.\n",
    "\n",
    "# What's Happening in BPE?\n",
    "\n",
    "BPE is a way to tokenize text by learning a vocabulary from a training corpus (a collection of text). It starts with individual characters and iteratively combines the most frequent pairs of adjacent symbols into new tokens, building a vocabulary that includes both characters and subwords (or even full words). This vocabulary is then used to tokenize new text, ensuring even unknown words can be handled.\n",
    "\n",
    "The process has two parts:\n",
    "1. **Token Learner**: Learns the vocabulary by merging frequent pairs.\n",
    "2. **Token Segmenter**: Uses the learned vocabulary to tokenize new text.\n",
    "\n",
    "Let's dive into the example from the book to clarify each part.\n",
    "\n",
    "## 1. Token Learner: Building the Vocabulary\n",
    "\n",
    "The Token Learner takes a training corpus and creates a vocabulary by repeatedly merging the most frequent pairs of adjacent symbols.\n",
    "\n",
    "### Input Corpus\n",
    "\n",
    "The example corpus is:\n",
    "\n",
    "```text\n",
    "5 low\n",
    "2 lowest\n",
    "6 newer\n",
    "3 wider\n",
    "2 new\n",
    "```\n",
    "\n",
    "- **Meaning**: The word `low` appears 5 times, `lowest` 2 times, `newer` 6 times, `wider` 3 times, and `new` 2 times.\n",
    "- **Step 1**: Split into Characters: Each word is broken into individual characters, with a special end-of-word symbol (`_` in this case, though some implementations use `</w>`). For example:\n",
    "  - `low` → `l o w _`\n",
    "  - `lowest` → `l o w e s t _`\n",
    "  - `newer` → `n e w e r _`\n",
    "  - `wider` → `w i d e r _`\n",
    "  - `new` → `n e w _`\n",
    "\n",
    "### Initial Vocabulary\n",
    "\n",
    "All unique characters in the corpus:\n",
    "\n",
    "```text\n",
    "Vocabulary: [, d, e, i, l, n, o, r, s, t, w]\n",
    "```\n",
    "\n",
    "(The comma `,` represents the space or separator, but it's not used in merging here since merges happen within words.)\n",
    "\n",
    "### How Merging Works\n",
    "\n",
    "The algorithm:\n",
    "1. Counts all pairs of adjacent symbols in the corpus.\n",
    "2. Merges the most frequent pair into a single token.\n",
    "3. Updates the corpus by replacing that pair with the new token.\n",
    "4. Adds the new token to the vocabulary.\n",
    "5. Repeats for a set number of merges (k).\n",
    "\n",
    "Let's walk through the merges step-by-step using the example.\n",
    "\n",
    "#### Merge 1: `e r` → `er`\n",
    "\n",
    "**Count Pairs**: Look at all adjacent pairs in the corpus:\n",
    "- In `newer` (`n e w e r *`): Pairs are `n e`, `e w`, `w e`, `e r`, `r *` (6 times, since newer appears 6 times).\n",
    "- In `wider` (`w i d e r *`): Pairs are `w i`, `i d`, `d e`, `e r`, `r *` (3 times).\n",
    "- Total counts: `e r` appears 6 (from newer) + 3 (from wider) = 9 times, the most frequent pair.\n",
    "\n",
    "**Merge**: Replace `e r` with `er` in the corpus:\n",
    "- `newer` → `n e w er _`\n",
    "- `wider` → `w i d er _`\n",
    "\n",
    "**Update Vocabulary**: Add `er`:\n",
    "\n",
    "```text\n",
    "Vocabulary: [, d, e, i, l, n, o, r, s, t, w, er]\n",
    "```\n",
    "\n",
    "#### Merge 2: `er *` → `er*`\n",
    "\n",
    "**Count Pairs**: Recount pairs in the updated corpus:\n",
    "- In `newer` (`n e w er *`): Pairs are `n e`, `e w`, `w er`, `er *` (6 times).\n",
    "- In `wider` (`w i d er *`): Pairs are `w i`, `i d`, `d er`, `er *` (3 times).\n",
    "- Total: `er _` appears 6 + 3 = 9 times, the most frequent.\n",
    "\n",
    "**Merge**: Replace `er *` with `er*`:\n",
    "- `newer` → `n e w er_`\n",
    "- `wider` → `w i d er_`\n",
    "\n",
    "**Update Vocabulary**:\n",
    "\n",
    "```text\n",
    "Vocabulary: [, d, e, i, l, n, o, r, s, t, w, er, er_]\n",
    "```\n",
    "\n",
    "#### Merge 3: `n e` → `ne`\n",
    "\n",
    "**Count Pairs**:\n",
    "- In `newer` (`n e w er_`): Pairs are `n e`, `e w`, `w er_` (6 times).\n",
    "- In `new` (`n e w *`): Pairs are `n e`, `e w`, `w *` (2 times).\n",
    "- Total: `n e` appears 6 + 2 = 8 times, the most frequent.\n",
    "\n",
    "**Merge**: Replace `n e` with `ne`:\n",
    "- `newer` → `ne w er_`\n",
    "- `new` → `ne w _`\n",
    "\n",
    "**Update Vocabulary**:\n",
    "\n",
    "```text\n",
    "Vocabulary: [, d, e, i, l, n, o, r, s, t, w, er, er_, ne]\n",
    "```\n",
    "\n",
    "#### Later Merges (summarized):\n",
    "\n",
    "- Merge `ne w` → `new`: Combines `ne` and `w` to form `new`.\n",
    "- Merge `l o` → `lo`: Combines `l` and `o`.\n",
    "- Merge `lo w` → `low`: Forms the full word `low`.\n",
    "- Merge `new er_` → `newer`: Forms the full word `newer`.\n",
    "- And so on, until k merges are done.\n",
    "\n",
    "### Final Vocabulary (after several merges):\n",
    "\n",
    "```text\n",
    "Vocabulary: [, d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer, ...]\n",
    "```\n",
    "\n",
    "- Contains original characters and new tokens (subwords like `er`, `ne`, or full words like `newer`, `low`).\n",
    "- The number of merges (k) determines how many new tokens are added.\n",
    "\n",
    "**Why This Matters**: The vocabulary now includes common subwords (`er`, `ne`) and words (`newer`, `low`), which can represent both known and unknown words in the test data.\n",
    "\n",
    "## 2. Token Segmenter: Tokenizing New Text\n",
    "\n",
    "The Token Segmenter:\n",
    "- Takes a new (test) word.\n",
    "- Splits it into individual characters.\n",
    "- Applies the merges learned during training in the same order.\n",
    "- Replaces pairs with merged tokens if they exist in the vocabulary.\n",
    "\n",
    "### Example 1: Tokenizing `newer`\n",
    "\n",
    "- **Input**: `newer` (a known word from the training corpus).\n",
    "- **Step 1**: Split into Characters: `n e w e r _`.\n",
    "- **Apply Merges** (in the order learned):\n",
    "  - Merge 1: `e r` → `er`. So: `n e w er _`.\n",
    "  - Merge 2: `er *` → `er*`. So: `n e w er_`.\n",
    "  - Merge 3: `n e` → `ne`. So: `ne w er_`.\n",
    "  - Later merge: `ne w` → `new`. So: `new er_`.\n",
    "  - Later merge: `new er_` → `newer`. So: `newer`.\n",
    "- **Result**: `newer` is tokenized as a single token `newer`, since it was learned as a full word.\n",
    "\n",
    "### Example 2: Tokenizing `lower` (unknown word)\n",
    "\n",
    "- **Input**: `lower` (not in training corpus).\n",
    "- **Step 1**: Split into Characters: `l o w e r _`.\n",
    "- **Apply Merges**:\n",
    "  - Merge 1: `e r` → `er`. So: `l o w er _`.\n",
    "  - Merge 2: `er *` → `er*`. So: `l o w er_`.\n",
    "  - Merge 3: `n e` → `ne` (not applicable, no `n e` in lower).\n",
    "  - Later merge: `l o` → `lo`. So: `lo w er_`.\n",
    "  - Later merge: `lo w` → `low`. So: `low er_`.\n",
    "  - No further merges apply (e.g., `low er_` isn't in the vocabulary).\n",
    "- **Result**: `lower` is tokenized as `low er_` (two tokens).\n",
    "\n",
    "**Key Point**: Even though `lower` wasn't in the training data, BPE tokenizes it using known subwords (`low` and `er_`), solving the unknown word problem.\n",
    "\n",
    "## Visualizing the Process\n",
    "\n",
    "Here's a simplified view of the corpus after a few merges:\n",
    "\n",
    "### Initial Corpus:\n",
    "\n",
    "```text\n",
    "5 l o w _\n",
    "2 l o w e s t _\n",
    "6 n e w e r _\n",
    "3 w i d e r _\n",
    "2 n e w _\n",
    "```\n",
    "\n",
    "### After Merge 1 (`e r` → `er`):\n",
    "\n",
    "```text\n",
    "5 l o w _\n",
    "2 l o w e s t _\n",
    "6 n e w er _\n",
    "3 w i d er _\n",
    "2 n e w _\n",
    "```\n",
    "\n",
    "### After Merge 2 (`er *` → `er*`):\n",
    "\n",
    "```text\n",
    "5 l o w _\n",
    "2 l o w e s t _\n",
    "6 n e w er_\n",
    "3 w i d er_\n",
    "2 n e w _\n",
    "```\n",
    "\n",
    "### After Merge 3 (`n e` → `ne`):\n",
    "\n",
    "```text\n",
    "5 l o w _\n",
    "2 l o w e s t _\n",
    "6 ne w er_\n",
    "3 w i d er_\n",
    "2 ne w _\n",
    "```\n",
    "\n",
    "### Tokenizing `lower`:\n",
    "\n",
    "- Start: `l o w e r _`\n",
    "- After merges: `low er_` (two tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a0375",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67832168",
   "metadata": {},
   "source": [
    "# What is Word Normalization?\n",
    "\n",
    "Word normalization is the process of transforming words or tokens into a standard format to ensure consistency in NLP tasks. The goal is to treat different surface forms of a word as the same when appropriate, improving generalization or matching in applications like search, speech recognition, or text analysis.\n",
    "\n",
    "## Key Example: Case Folding\n",
    "\n",
    "### Definition\n",
    "Case folding is the simplest form of normalization, where all letters in a word are converted to lowercase (or sometimes uppercase).\n",
    "\n",
    "### Example\n",
    "The words `Woodchuck` and `woodchuck` are treated as identical after case folding to `woodchuck`.\n",
    "\n",
    "### Why It's Useful\n",
    "\n",
    "- Helps generalization in tasks like information retrieval (e.g., search engines) or speech recognition, where `Woodchuck` and `woodchuck` should match the same concept.\n",
    "- Reduces vocabulary size by collapsing case variations.\n",
    "\n",
    "### When It's Not Used\n",
    "\n",
    "- In tasks like sentiment analysis, information extraction, or machine translation, case can carry meaning. For example:\n",
    "  - `US` (the country) vs. `us` (the pronoun) have different meanings.\n",
    "  - Keeping case distinctions can help models differentiate these terms.\n",
    "- Some systems maintain both cased (preserving upper/lowercase) and uncased (all lowercase) versions of language models to balance flexibility.\n",
    "\n",
    "## Other Normalization Examples\n",
    "\n",
    "Beyond case folding, normalization may involve standardizing different spellings or formats of the same concept:\n",
    "\n",
    "- **Example**: Normalizing `USA` and `US` to a single form (e.g., `US`) for consistency in information retrieval or extraction.\n",
    "- **Example**: Standardizing `uh-huh` and `uhhuh` to a single form (e.g., `uhhuh`).\n",
    "- **Trade-Off**: Normalization simplifies processing but may lose spelling-specific information (e.g., stylistic differences in `uh-huh` vs. `uhhuh`).\n",
    "\n",
    "## Relation to BPE\n",
    "\n",
    "- Systems using Byte-Pair Encoding (BPE) (from Section 2.5.2) often rely on subword tokenization and may not need further normalization, as BPE already handles variations by breaking words into subword units.\n",
    "- Other NLP systems, however, may apply normalization (like case folding or standardizing US/USA) to ensure consistency before or after tokenization.\n",
    "\n",
    "# Lemmatization\n",
    "\n",
    "Lemmatization is a more advanced form of normalization that involves reducing different morphological forms of a word to a single base form, called the lemma. The lemma is the dictionary form or root of a word, ignoring inflectional variations (e.g., tense, number, case).\n",
    "\n",
    "## Why Lemmatization?\n",
    "\n",
    "### Purpose\n",
    "Ensures that different forms of a word are treated as the same concept, especially in languages with complex morphology.\n",
    "\n",
    "### Examples\n",
    "- **English**: `am`, `are`, `is` → lemma `be`.\n",
    "- **English**: `dinner`, `dinners` → lemma `dinner`.\n",
    "- **Polish**: `Warszawa` (subject), `w Warszawie` (in Warsaw), `do Warszawy` (to Warsaw) → lemma `Warszawa`.\n",
    "\n",
    "### Use Case\n",
    "In web search, lemmatizing `woodchucks` to `woodchuck` ensures search results include pages with either form. In Polish, lemmatizing `Warszawie` to `Warszawa` helps retrieve all mentions of Warsaw regardless of grammatical form.\n",
    "\n",
    "## How Lemmatization Works\n",
    "\n",
    "### Morphological Parsing\n",
    "Lemmatization often relies on analyzing a word's morphemes, the smallest meaning-bearing units in a language.\n",
    "\n",
    "#### Types of Morphemes\n",
    "- **Stem**: The core meaning-bearing part of a word (e.g., `walk` in `walking`).\n",
    "- **Affixes**: Prefixes, suffixes, or other modifications (e.g., `-ing`, `-s`).\n",
    "\n",
    "#### Example\n",
    "The word `woodchucks` has:\n",
    "- **Stem**: `woodchuck`.\n",
    "- **Suffix**: `-s` (plural marker).\n",
    "- **Lemma**: `woodchuck` (the singular, base form).\n",
    "\n",
    "### Process\n",
    "A lemmatizer uses a dictionary or morphological rules to map a word to its lemma. For example:\n",
    "- **Input**: `He is reading detective stories.`\n",
    "- **Lemmatized**: `He be read detective story` (each word is reduced to its lemma).\n",
    "\n",
    "### Tools\n",
    "Advanced lemmatizers use:\n",
    "- **Morphological parsers**: Break down words into stems and affixes.\n",
    "- **Dictionaries**: Map inflected forms to their lemmas (e.g., WordNet for English).\n",
    "- **Rules**: Handle regular patterns (e.g., `-s` for plurals in English) or language-specific morphology (e.g., Polish case endings).\n",
    "\n",
    "## Challenges in Lemmatization\n",
    "\n",
    "- Requires knowledge of the language's morphology (e.g., English is simpler, but Polish has complex case systems).\n",
    "- Needs context for ambiguous words (e.g., `saw` could be the lemma for `saw` (past tense of see) or `saw` (a cutting tool)).\n",
    "- More computationally intensive than simple normalization like case folding.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "Stemming is a simpler, cruder alternative to lemmatization for reducing words to a base form, called the stem. Unlike lemmatization, which produces a valid dictionary word (the lemma) using morphological analysis, stemming uses heuristic rules to strip off affixes (e.g., prefixes or suffixes), often resulting in a stem that may not be a valid word.\n",
    "\n",
    "## What is Stemming?\n",
    "\n",
    "### Definition\n",
    "Stemming chops off word-final affixes (like `-s`, `-ing`) to reduce words to a common base form, ignoring complex morphological rules.\n",
    "\n",
    "### Goal\n",
    "Collapse different forms of a word to a single representation to improve generalization in tasks like information retrieval.\n",
    "\n",
    "### Example\n",
    "- `cats` → stem `cat` (removes `-s`).\n",
    "- `motoring` → stem `motor` (removes `-ing`).\n",
    "- `grasses` → stem `grass` (replaces `-sses` with `-ss`).\n",
    "\n",
    "## The Porter Stemmer\n",
    "\n",
    "The Porter Stemmer (Porter, 1980) is a classic stemming algorithm mentioned in the text. It's widely used due to its simplicity and effectiveness in English.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The Porter Stemmer applies a series of rewrite rules in sequential passes. Each rule transforms the word by removing or replacing affixes, and the output of one pass becomes the input for the next.\n",
    "\n",
    "#### Sample Rules (from the text and the referenced website)\n",
    "- `ATIONAL` → `ATE`: `relational` → `relate` (removes `-ational`, adds `-ate`).\n",
    "- `ING` → `ε` (if the stem contains a vowel): `motoring` → `motor` (removes `-ing`).\n",
    "- `SSES` → `SS`: `grasses` → `grass` (replaces `-sses` with `-ss`).\n",
    "\n",
    "Rules are applied iteratively, and each rule checks conditions (e.g., the stem must contain a vowel for `ING` → `ε`).\n",
    "\n",
    "### Example from the Text\n",
    "\n",
    "**Input paragraph**:\n",
    "\n",
    "```text\n",
    "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things—names and heights and soundings—with the single exception of the red crosses and the written notes.\n",
    "```\n",
    "\n",
    "**Stemmed output** (using the Porter Stemmer):\n",
    "\n",
    "```text\n",
    "Thi wa not the map we found in Billi Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red cross and the written note\n",
    "```\n",
    "\n",
    "#### Observations\n",
    "- `This` → `Thi` (over-stemming, removes `-s` incorrectly).\n",
    "- `was` → `wa` (removes `-s`).\n",
    "- `accurate` → `accur` (removes `-ate`).\n",
    "- `copy` → `copi` (removes `-y`, possibly incorrect).\n",
    "- `complete` → `complet` (removes `-e`).\n",
    "- `things` → `thing` (removes `-s`).\n",
    "- `names` → `name` (removes `-s`).\n",
    "- `singl` → `singl` (from `single`, removes `-e`).\n",
    "- Errors like `Thi` and `copi` show stemming's limitations (see below).\n",
    "\n",
    "## Advantages of Stemming\n",
    "\n",
    "- **Simplicity**: Faster and less resource-intensive than lemmatization, as it relies on rules rather than dictionaries or morphological parsers.\n",
    "- **Generalization**: Collapses variants of a word (e.g., `running`, `runs` → `run`), useful for search engines where exact word forms are less important.\n",
    "\n",
    "## Limitations of Stemming\n",
    "\n",
    "- **Over-Generalization**: Incorrectly stems words, e.g., `policy` → `polic` (confused with `police`), as noted in the text.\n",
    "- **Under-Generalization**: Fails to stem related forms, e.g., `European` → `Europe` (not recognized as related to `Europe`).\n",
    "- **Non-Word Stems**: Produces stems that aren't valid words, e.g., `better` → `bett`, unlike lemmatization's valid lemmas.\n",
    "- **Modern Usage**: Less common in modern NLP systems (e.g., those using BPE or advanced lemmatization) due to these errors, but still used in lightweight applications like search.\n",
    "\n",
    "## Stemming vs. Lemmatization\n",
    "\n",
    "### Lemmatization\n",
    "- Uses linguistic knowledge (dictionaries, morphological parsers).\n",
    "- Produces valid words (e.g., `running` → `run`, `better` → `good`).\n",
    "- More accurate but computationally expensive.\n",
    "\n",
    "### Stemming\n",
    "- Uses simple rules, no linguistic knowledge.\n",
    "- Produces stems that may not be words (e.g., `better` → `bett`).\n",
    "- Faster but less precise, prone to errors.\n",
    "\n",
    "### Example\n",
    "- **Input**: `cats`, `running`, `better`.\n",
    "- **Lemmatization**: `cat`, `run`, `good`.\n",
    "- **Stemming (Porter)**: `cat`, `run`, `bett`.\n",
    "\n",
    "# Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is the process of dividing a text into individual sentences, a critical step in NLP for tasks like machine translation, text summarization, or parsing, where sentences are processed as units.\n",
    "\n",
    "## Why Sentence Segmentation?\n",
    "\n",
    "- Many NLP systems operate on sentences, not raw text.\n",
    "- Accurate sentence boundaries ensure correct input for downstream tasks (e.g., parsing, translation).\n",
    "- Challenges arise due to ambiguous punctuation, particularly the period (`.`).\n",
    "\n",
    "## Key Cues for Sentence Segmentation\n",
    "\n",
    "### Punctuation Markers\n",
    "\n",
    "#### Unambiguous\n",
    "Question marks (`?`) and exclamation points (`!`) are clear sentence boundary indicators.\n",
    "\n",
    "#### Ambiguous\n",
    "Periods (`.`) can mark:\n",
    "- **Sentence boundaries**: `I love NLP. It's fun.`\n",
    "- **Abbreviations**: `Mr. Smith` (not a sentence end).\n",
    "- **Both**: `The meeting is at Inc.` (period marks abbreviation and sentence end).\n",
    "\n",
    "### Context\n",
    "The role of a period depends on the surrounding text (e.g., `Inc.` vs. a sentence-ending period).\n",
    "\n",
    "## How Sentence Segmentation Works\n",
    "\n",
    "### General Approach\n",
    "\n",
    "1. **Identify Periods**: Determine whether a period is part of a word (e.g., abbreviation like `Mr.`) or a sentence boundary.\n",
    "2. **Use Rules or Machine Learning**:\n",
    "   - **Rule-Based**: Apply hand-crafted rules, often integrated with word tokenization.\n",
    "   - **Machine Learning**: Train models to classify periods as sentence boundaries or not (e.g., Kiss and Strunk, 2006).\n",
    "3. **Abbreviation Dictionaries**: Use lists of common abbreviations (`Mr.`, `Inc.`, `Dr.`) to avoid misinterpreting periods.\n",
    "\n",
    "### Stanford CoreNLP Example (from the text)\n",
    "\n",
    "- Uses a rule-based approach tied to tokenization.\n",
    "- A sentence ends when a punctuation mark (`.`, `!`, `?`) is not part of a token (e.g., not in `Mr.` or `3.14`).\n",
    "- Optionally, sentences may end with closing quotes or brackets (e.g., `He said, \"Hello.\"` → two sentences: `He said` and `Hello`).\n",
    "\n",
    "### Joint Tokenization\n",
    "Sentence and word tokenization are often done together, as periods affect both (e.g., deciding if `Inc.` is one token or ends a sentence).\n",
    "\n",
    "## Challenges in Sentence Segmentation\n",
    "\n",
    "- **Ambiguous Periods**: `Inc.` can be an abbreviation or a sentence end, requiring context to disambiguate.\n",
    "- **Complex Cases**: Nested punctuation (e.g., `He said, \"Stop!\"`) or lists (e.g., `Items: a. Apple b. Banana`) complicates boundary detection.\n",
    "- **Language-Specific Issues**: Some languages lack clear punctuation or use different conventions, requiring tailored approaches.\n",
    "\n",
    "## Example\n",
    "\n",
    "**Input text**:\n",
    "\n",
    "```text\n",
    "Mr. Smith went to Inc. He works there.\n",
    "```\n",
    "\n",
    "### Naive Approach\n",
    "Split on every period → incorrect:\n",
    "- `Mr.` (not a sentence).\n",
    "- `Smith went to Inc.` (correct).\n",
    "- `He works there.` (correct).\n",
    "\n",
    "### Smart Approach (e.g., CoreNLP)\n",
    "Recognize `Mr.` and `Inc.` as abbreviation tokens using a dictionary or rules.\n",
    "\n",
    "**Output**:\n",
    "- **Sentence 1**: `Mr. Smith went to Inc.`\n",
    "- **Sentence 2**: `He works there.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0da6da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e91f426",
   "metadata": {},
   "source": [
    "# Minimum Edit Distance\n",
    "\n",
    "## What is Minimum Edit Distance?\n",
    "\n",
    "Minimum edit distance is a measure of how different two strings are, defined as the minimum number of operations (insertions, deletions, or substitutions) needed to transform one string into another. It quantifies string similarity, which is useful in many NLP tasks where we need to compare or align text.\n",
    "\n",
    "## Why is it Useful?\n",
    "\n",
    "The text highlights three key applications:\n",
    "\n",
    "### 1. Spelling Correction\n",
    "- **Example**: A user types `graffe`. The system suggests `giraffe` (differs by one letter: a → i) over `grail` or `graf` (more differences) because it has a lower edit distance.\n",
    "\n",
    "### 2. Coreference Resolution\n",
    "- **Example**: Deciding if \"Stanford Arizona Cactus Garden\" and \"Stanford University Arizona Cactus Garden\" refer to the same entity. Their similarity (differing by one word) suggests they might be coreferent.\n",
    "\n",
    "### 3. Speech Recognition Evaluation\n",
    "- Compares a system's transcript to a reference transcript to measure accuracy. Fewer differences (lower edit distance) indicate better performance.\n",
    "\n",
    "## Key Operations\n",
    "\n",
    "The operations used to transform one string into another are:\n",
    "\n",
    "- **Insertion**: Add a character (e.g., `cat` → `cast` by inserting `s`)\n",
    "- **Deletion**: Remove a character (e.g., `cast` → `cat` by deleting `s`)\n",
    "- **Substitution**: Replace one character with another (e.g., `cat` → `cut` by substituting `a` → `u`)\n",
    "\n",
    "## Levenshtein Distance\n",
    "\n",
    "The text describes two versions of the Levenshtein distance, a common edit distance metric:\n",
    "\n",
    "### 1. Standard Levenshtein\n",
    "- **Insertion**: Cost = 1\n",
    "- **Deletion**: Cost = 1\n",
    "- **Substitution**: Cost = 1 (0 if substituting a character with itself, e.g., `t` → `t`)\n",
    "\n",
    "### 2. Alternative Levenshtein (used in the algorithm example)\n",
    "- **Insertion**: Cost = 1\n",
    "- **Deletion**: Cost = 1\n",
    "- **Substitution**: Cost = 2 (equivalent to one insertion + one deletion; 0 if characters are identical)\n",
    "\n",
    "### Example: Transform `intention` to `execution` (using alternative Levenshtein)\n",
    "- **Operations**: Delete `i`, substitute `n` → `e`, substitute `t` → `x`, insert `u`, substitute `n` → `c`\n",
    "- **Cost**: 1 (delete) + 2 (sub n → e) + 2 (sub t → x) + 1 (insert u) + 2 (sub n → c) = 8\n",
    "\n",
    "## Alignment\n",
    "\n",
    "An alignment visualizes how one string transforms into another by matching characters and indicating operations:\n",
    "\n",
    "**Example** (from Fig. 2.14):\n",
    "\n",
    "```text\n",
    "INTE*NTION\n",
    "||| |||||\n",
    "*EXECUTION\n",
    "d s s i s\n",
    "```\n",
    "\n",
    "- **d**: Delete I (no match in execution)\n",
    "- **s**: Substitute N → E, T → X, N → C\n",
    "- **i**: Insert U\n",
    "- **Matches**: T, I, O, N align directly (cost = 0 for identical characters)\n",
    "\n",
    "This alignment shows the operations needed to transform `intention` into `execution`.\n",
    "\n",
    "## The Minimum Edit Distance Algorithm\n",
    "\n",
    "Computing the minimum edit distance naively (trying all possible operation sequences) is inefficient due to the vast number of possible paths. Instead, the algorithm uses **dynamic programming**, a method that breaks a problem into smaller subproblems, stores their solutions, and combines them to solve the larger problem efficiently.\n",
    "\n",
    "### Intuition of Dynamic Programming\n",
    "\n",
    "- **Concept**: If you're transforming `intention` to `execution` via an intermediate string (e.g., `exention`), the optimal path to `exention` must be part of the overall optimal path. Otherwise, a shorter path to `exention` would imply a shorter overall path, contradicting optimality.\n",
    "- **Approach**: Build a table (matrix) to store the edit distances for all prefixes of the source and target strings, computing larger distances from smaller ones.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "- **Input**: Source string X of length n (e.g., `intention`), target string Y of length m (e.g., `execution`)\n",
    "- **Goal**: Compute D[n, m], the minimum edit distance between X and Y\n",
    "- **Subproblem**: D[i, j] is the minimum edit distance between the first i characters of X (X[1..i]) and the first j characters of Y (Y[1..j])\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "The algorithm (Fig. 2.17) uses a dynamic programming matrix D of size (n+1) × (m+1):\n",
    "\n",
    "### 1. Initialization\n",
    "- **D[0, 0] = 0** (empty source to empty target requires no operations)\n",
    "- **First row (D[i, 0])**: Distance from X[1..i] to an empty string = i deletions\n",
    "  - E.g., D[1, 0] = 1 (delete one character from X)\n",
    "- **First column (D[0, j])**: Distance from empty string to Y[1..j] = j insertions\n",
    "  - E.g., D[0, 1] = 1 (insert one character from Y)\n",
    "\n",
    "### 2. Recurrence Relation\n",
    "For each cell D[i, j], compute the minimum cost of transforming X[1..i] into Y[1..j] by considering three possible operations:\n",
    "\n",
    "- **Deletion**: Remove X[i]. Cost = D[i-1, j] + del-cost(X[i])\n",
    "- **Insertion**: Add Y[j]. Cost = D[i, j-1] + ins-cost(Y[j])\n",
    "- **Substitution**: Replace X[i] with Y[j]. Cost = D[i-1, j-1] + sub-cost(X[i], Y[j])\n",
    "  - If X[i] = Y[j], sub-cost = 0\n",
    "  - If X[i] ≠ Y[j], sub-cost = 2 (in alternative Levenshtein)\n",
    "\n",
    "**Formula (Eq. 2.24):**\n",
    "\n",
    "```text\n",
    "D[i, j] = min(\n",
    "    D[i-1, j] + 1,              # Deletion\n",
    "    D[i, j-1] + 1,              # Insertion\n",
    "    D[i-1, j-1] + (2 if X[i] ≠ Y[j] else 0)  # Substitution\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Termination\n",
    "D[n, m] is the minimum edit distance.\n",
    "\n",
    "## Example: `intention` → `execution`\n",
    "\n",
    "- **Source (X)**: `intention` (n=9)\n",
    "- **Target (Y)**: `execution` (m=9)\n",
    "- **Matrix** (Fig. 2.18, partially reproduced):\n",
    "\n",
    "```text\n",
    "Src\\Tar | # | e | x | e | c | u | t | i | o | n\n",
    "--------|---|---|---|---|---|---|---|---|---|---\n",
    "#       | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n",
    "i       | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 6 | 7 | 8\n",
    "n       | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 7 | 8 | 7\n",
    "t       | 3 | 4 | 5 | 6 | 7 | 8 | 7 | 8 | 9 | 8\n",
    "e       | 4 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11\n",
    "n       | 5 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |10\n",
    "t       | 6 | 5 | 6 | 7 | 8 | 9 | 8 | 9 |10 |11\n",
    "i       | 7 | 6 | 7 | 8 | 9 |10 | 9 | 8 | 9 |10\n",
    "o       | 8 | 7 | 8 | 9 |10 |11 |10 | 9 | 8 | 9\n",
    "n       | 9 | 8 | 9 |10 |11 |12 |11 |10 | 9 | 8\n",
    "```\n",
    "\n",
    "### Key Cells Analysis\n",
    "- **D[0, 0] = 0** (empty to empty)\n",
    "- **D[1, 0] = 1** (delete i from i)\n",
    "- **D[0, 1] = 1** (insert e to empty)\n",
    "- **D[1, 1]**: Compare i and e:\n",
    "  - Delete i: D[0, 1] + 1 = 1 + 1 = 2\n",
    "  - Insert e: D[1, 0] + 1 = 1 + 1 = 2\n",
    "  - Substitute i → e: D[0, 0] + 2 = 0 + 2 = 2\n",
    "  - D[1, 1] = min(2, 2, 2) = 2\n",
    "- **D[9, 9] = 8** (final edit distance)\n",
    "\n",
    "### Result\n",
    "**Levenshtein distance = 8** (using cost of 2 for substitutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049916e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
