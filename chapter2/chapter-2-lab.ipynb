{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b79c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/musty/anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/musty/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/musty/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/musty/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/musty/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/musty/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/musty/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/musty/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/musty/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c14eec",
   "metadata": {},
   "source": [
    "## Using Pretrained Tokenizers (GPT-2 and BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a27252",
   "metadata": {},
   "source": [
    "\n",
    "### Purpose\n",
    "Demonstrates how real-world tokenizers (GPT-2 and BERT) handle tokenization and normalization, providing a baseline for comparison with the custom BPE implementation.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### `demonstrate_pretrained_tokenizers()`\n",
    "\n",
    "#### Tokenization (GPT-2)\n",
    "Uses BPE to split text into tokens, showing subword units for rare words (e.g., videoconference â†’ vide, ocon, ference).\n",
    "\n",
    "#### Encoding\n",
    "Converts tokens to vocabulary IDs, a step used in NLP models.\n",
    "\n",
    "#### Normalization (BERT)\n",
    "Applies case folding (lowercase), aligning with Section 2.6.\n",
    "\n",
    "#### Pre-Tokenization\n",
    "Shows how text is split into words before BPE or WordPiece, as described in Section 2.5.2.\n",
    "\n",
    "### Relation to Chapter\n",
    "\n",
    "#### Section 2.5.2 (BPE)\n",
    "GPT-2's tokenizer uses BPE, breaking words into subwords to handle unknown words, as shown with videoconference.\n",
    "\n",
    "#### Section 2.6 (Normalization)\n",
    "BERT's case folding (Hello â†’ hello) demonstrates normalization for generalization, as discussed in the chapter.\n",
    "\n",
    "#### Section 2.7 (Sentence Segmentation)\n",
    "Implicitly handled by treating punctuation (., ,) as separate tokens, aligning with the chapter's discussion of punctuation as boundary markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef699c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcb3f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample corpus for BPE training\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "    \"Thisit\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "141d4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer (uses BPE)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "254a1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test prompt\n",
    "prompt = \"Hello, this is John Doe. I am doing a videoconference from my  office\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d6af696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Tokenization: ['Hello', ',', 'Ä this', 'Ä is', 'Ä John', 'Ä Doe', '.', 'Ä I', 'Ä am', 'Ä doing', 'Ä a', 'Ä vide', 'ocon', 'ference', 'Ä from', 'Ä my', 'Ä ', 'Ä office']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt using GPT-2\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(prompt)\n",
    "print(\"GPT-2 Tokenization:\", gpt2_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0210a",
   "metadata": {},
   "source": [
    "Note: 'Ä ' denotes a space (GPT-2 convention); 'videoconference' is split into subwords ('vide', 'ocon', 'ference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "674a4f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Encoded IDs: tensor([[15496,    11,   428,   318,  1757, 31780,    13,   314,   716,  1804,\n",
      "           257, 18784, 36221,  4288,   422,   616,   220,  2607]])\n"
     ]
    }
   ],
   "source": [
    "# Encode tokens to vocabulary IDs (returns PyTorch tensor)\n",
    "gpt2_encoded = gpt2_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"GPT-2 Encoded IDs:\", gpt2_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd69a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer (uses WordPiece, similar to BPE)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9128738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Normalized Prompt: hello, this is john doe. i am doing a videoconference from my  office\n"
     ]
    }
   ],
   "source": [
    "# Normalize the prompt using BERT's normalizer (case folding)\n",
    "normalized_prompt = bert_tokenizer.backend_tokenizer.normalizer.normalize_str(prompt)\n",
    "print(\"BERT Normalized Prompt:\", normalized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b97fc",
   "metadata": {},
   "source": [
    "Note: Converts to lowercase, demonstrating case folding (Section 2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82607aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Normalizer Methods: ['__bool__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n",
      "BERT Normalizer Methods: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'clean_text', 'custom', 'handle_chinese_chars', 'lowercase', 'normalize', 'normalize_str', 'strip_accents']\n"
     ]
    }
   ],
   "source": [
    "# Inspect normalizer methods for GPT-2 and BERT\n",
    "print(\"GPT-2 Normalizer Methods:\", [i for i in dir(gpt2_tokenizer.backend_tokenizer.normalizer)])\n",
    "print(\"BERT Normalizer Methods:\", [i for i in dir(bert_tokenizer.backend_tokenizer.normalizer)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40dedf",
   "metadata": {},
   "source": [
    "Note: BERT includes 'lowercase', 'strip_accents', etc., showing advanced normalization (Section 2.6)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ba04bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Pre-Tokenization: [('Hello', (0, 5)), (',', (5, 6)), ('this', (7, 11)), ('is', (12, 14)), ('John', (15, 19)), ('Doe', (20, 23)), ('.', (23, 24)), ('I', (25, 26)), ('am', (27, 29)), ('doing', (30, 35)), ('a', (36, 37)), ('videoconference', (38, 53)), ('from', (54, 58)), ('my', (59, 61)), ('office', (63, 69))]\n",
      "GPT-2 Pre-Tokenization: [('Hello', (0, 5)), (',', (5, 6)), ('Ä this', (6, 11)), ('Ä is', (11, 14)), ('Ä John', (14, 19)), ('Ä Doe', (19, 23)), ('.', (23, 24)), ('Ä I', (24, 26)), ('Ä am', (26, 29)), ('Ä doing', (29, 35)), ('Ä a', (35, 37)), ('Ä videoconference', (37, 53)), ('Ä from', (53, 58)), ('Ä my', (58, 61)), ('Ä ', (61, 62)), ('Ä office', (62, 69))]\n"
     ]
    }
   ],
   "source": [
    "# Pre-tokenize the prompt using BERT and GPT-2\n",
    "bert_pre_tokens = bert_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(prompt)\n",
    "gpt2_pre_tokens = gpt2_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(prompt)\n",
    "print(\"BERT Pre-Tokenization:\", bert_pre_tokens)\n",
    "print(\"GPT-2 Pre-Tokenization:\", gpt2_pre_tokens)\n",
    "# Output: Lists of (word, (start, end)) tuples, splitting on spaces/punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d68d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81e7a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec09ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Emails: ['crazy_coder99@gmail.com']\n",
      "Extracted Hashtags: ['#speedster', '#PythonRocks']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Contact me at crazy_coder99@gmail.com or visit my profile at http://example.com!\n",
    "Iâ€™ve been running, RUNS!! He ran quicklyâ€”faster than anyone! #speedster #PythonRocks ðŸš€ðŸš€ðŸš€\n",
    "\"\"\"\n",
    "\n",
    "# STEP 1: REGULAR EXPRESSION - Extract emails and hashtags\n",
    "emails = re.findall(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b', text)\n",
    "hashtags = re.findall(r'#\\w+', text)\n",
    "\n",
    "print(\"Extracted Emails:\", emails)\n",
    "print(\"Extracted Hashtags:\", hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b010387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: TOKENIZATION - Sentence and Word Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ff88b",
   "metadata": {},
   "source": [
    "word_tokenize implements rule-based word tokenization, splitting on whitespace and punctuation, unlike BPEâ€™s subword approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "693c9c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nContact me at crazy_coder99@gmail.com or visit my profile at http://example.com!',\n",
       " 'Iâ€™ve been running, RUNS!!',\n",
       " 'He ran quicklyâ€”faster than anyone!',\n",
       " '#speedster #PythonRocks ðŸš€ðŸš€ðŸš€']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7147f430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contact',\n",
       " 'me',\n",
       " 'at',\n",
       " 'crazy_coder99',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " 'or',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'profile',\n",
       " 'at',\n",
       " 'http',\n",
       " ':',\n",
       " '//example.com',\n",
       " '!',\n",
       " 'I',\n",
       " 'â€™',\n",
       " 've',\n",
       " 'been',\n",
       " 'running',\n",
       " ',',\n",
       " 'RUNS',\n",
       " '!',\n",
       " '!',\n",
       " 'He',\n",
       " 'ran',\n",
       " 'quicklyâ€”faster',\n",
       " 'than',\n",
       " 'anyone',\n",
       " '!',\n",
       " '#',\n",
       " 'speedster',\n",
       " '#',\n",
       " 'PythonRocks',\n",
       " 'ðŸš€ðŸš€ðŸš€']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3841f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contact',\n",
       " 'me',\n",
       " 'at',\n",
       " 'crazy_coder99',\n",
       " 'gmail.com',\n",
       " 'or',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'profile',\n",
       " 'at',\n",
       " 'http',\n",
       " '//example.com',\n",
       " 'i',\n",
       " 'â€™',\n",
       " 've',\n",
       " 'been',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'he',\n",
       " 'ran',\n",
       " 'quicklyâ€”faster',\n",
       " 'than',\n",
       " 'anyone',\n",
       " 'speedster',\n",
       " 'pythonrocks',\n",
       " 'ðŸš€ðŸš€ðŸš€']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applies case folding (lowercase) and filters out punctuation.Output: Includes emails, URLs, and emojis, but removes commas, periods, etc.\n",
    "\n",
    "normalized_words = [word.lower() for word in words if word not in string.punctuation]\n",
    "normalized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57e5395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercased Words: ['contact', 'me', 'at', 'crazy_coder99', 'gmail.com', 'or', 'visit', 'my', 'profile', 'at', 'http', '//example.com', 'i', 'â€™', 've', 'been', 'running', 'runs', 'he', 'ran', 'quicklyâ€”faster', 'than', 'anyone', 'speedster', 'pythonrocks', 'ðŸš€ðŸš€ðŸš€']\n",
      "\n",
      "Stemmed Words: ['contact', 'me', 'at', 'crazy_coder99', 'gmail.com', 'or', 'visit', 'my', 'profil', 'at', 'http', '//example.com', 'i', 'â€™', 've', 'been', 'run', 'run', 'he', 'ran', 'quicklyâ€”fast', 'than', 'anyon', 'speedster', 'pythonrock', 'ðŸš€ðŸš€ðŸš€']\n",
      "\n",
      "Lemmatized Words: ['contact', 'me', 'at', 'crazy_coder99', 'gmail.com', 'or', 'visit', 'my', 'profile', 'at', 'http', '//example.com', 'i', 'â€™', 've', 'be', 'run', 'run', 'he', 'run', 'quicklyâ€”faster', 'than', 'anyone', 'speedster', 'pythonrocks', 'ðŸš€ðŸš€ðŸš€']\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in normalized_words]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in normalized_words]\n",
    "\n",
    "print(\"\\nLowercased Words:\", normalized_words)\n",
    "print(\"\\nStemmed Words:\", stems)\n",
    "print(\"\\nLemmatized Words:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fd8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57168698",
   "metadata": {},
   "source": [
    "## Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce71c04",
   "metadata": {},
   "source": [
    "### Spelling Correction and Alignment\n",
    "\n",
    "#### Step 1: Spelling Correction\n",
    "\n",
    "#### Function: `correct_spelling`\n",
    "Uses edit_distance to find the candidate word with the minimum edit distance to a misspelled word (e.g., Thisit â†’ This it).\n",
    "\n",
    "#### Relation to BPE Code\n",
    "Corrects Thisit from the BPE corpus, which could be a typo.\n",
    "\n",
    "#### Relation to Chapter\n",
    "Implements Section 2.8's spelling correction example (e.g., graffe â†’ giraffe).\n",
    "\n",
    "#### Step 2: Alignment\n",
    "\n",
    "#### Function: `get_edit_alignment`\n",
    "Uses edit_distance_align to compute the edit distance and alignment, visualizing operations (insertions, deletions, substitutions) as in Fig. 2.14.\n",
    "\n",
    "#### Relation to BPE Code\n",
    "Shows how Thisit transforms to This it (insert space).\n",
    "\n",
    "#### Relation to Chapter\n",
    "Matches the chapter's alignment for intention â†’ execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9760851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance, edit_distance_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4061f081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/musty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # For word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7937972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"Thisit\", \"This it\", substitution_cost=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "454dde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Minimum Edit Distance for Spelling Correction ---\n",
    "def correct_spelling(word, candidates, substitution_cost=1):\n",
    "    \"\"\"\n",
    "    Corrects a misspelled word by finding the candidate with minimum edit distance.\n",
    "    Relates to Section 2.8 (Minimum Edit Distance) of the book.\n",
    "\n",
    "    Args:\n",
    "        word (str): The potentially misspelled word.\n",
    "        candidates (list): List of correct words to compare against.\n",
    "        substitution_cost (int): Cost of substitution (1 for standard Levenshtein, 2 for alternative).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Corrected word and its edit distance.\n",
    "    \"\"\"\n",
    "    distances = [(candidate, edit_distance(word, candidate, substitution_cost=substitution_cost))\n",
    "                 for candidate in candidates]\n",
    "    return min(distances, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "183a1af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Correction for 'Thisit':\n",
      "Corrected Word: This it, Edit Distance: 1\n"
     ]
    }
   ],
   "source": [
    "candidates = [\"This it\", \"This\", \"That\", \"Thus\"]\n",
    "misspelled = \"Thisit\"\n",
    "corrected_word, distance = correct_spelling(misspelled, candidates, substitution_cost=2)\n",
    "print(f\"Spelling Correction for '{misspelled}':\")\n",
    "print(f\"Corrected Word: {corrected_word}, Edit Distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "870d4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Alignment for Visualization ---\n",
    "def get_edit_alignment(source, target, substitution_cost=2):\n",
    "    \"\"\"\n",
    "    Computes the edit distance and alignment between two strings.\n",
    "    Relates to Section 2.8 (Alignment for Minimum Edit Distance).\n",
    "\n",
    "    Args:\n",
    "        source (str): Source string.\n",
    "        target (str): Target string.\n",
    "        substitution_cost (int): Cost of substitution.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Edit distance and list of alignment operations.\n",
    "    \"\"\"\n",
    "    distance = edit_distance(source, target, substitution_cost=substitution_cost)\n",
    "    alignment = edit_distance_align(source, target, substitution_cost=substitution_cost)\n",
    "    return distance, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e40594b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment for 'Thisit' -> 'This it':\n",
      "Edit Distance: 1\n",
      "Alignment: [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (4, 5), (5, 6), (6, 7)]\n"
     ]
    }
   ],
   "source": [
    "# Example: Align 'Thisit' with 'This it'\n",
    "distance, alignment = get_edit_alignment(\"Thisit\", \"This it\", substitution_cost=2)\n",
    "print(f\"\\nAlignment for 'Thisit' -> 'This it':\")\n",
    "print(f\"Edit Distance: {distance}\")\n",
    "print(f\"Alignment: {alignment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b1989",
   "metadata": {},
   "source": [
    "Note: Alignment shows character mappings; (6, 7) indicates inserting a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec4e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
